\chapter{Description of the environments}
\label{a:description}

In this appendix we will summarize all the details regarding all the environments used in the experiments of this work.


\section{LQR}
\label{a:lqr_description}
The \glsfirst{lqr} domain is the classical control theory task of stabilizing a linear system (with a linear feedback controller) under a quadratic cost objective function. The system dynamics are:
\begin{equation*}
x(t+1) = Ax(t) + Bu(t).
\end{equation*}
In our experiments we used the identity matrix for $A$ and $B$.
The reward function is:
\begin{equation*}
r(t) = -x(t)^TQx(t) - u(t)^TRu(t).
\end{equation*}
The discount factor for every experiment using this environment is set to $\gamma=0.9$.
This environments has no absorbing states, but we set an horizon of 50 steps for each episode.

\section{NLS}
\label{a:nls_description}
The \gls{nls} environment is a 2D nonlinear system defined in~\cite{vlassis2009learning}.
The system dynamics are:
\begin{align*}
x_1(t+1)&=x_1(t)-0.2\,x_2(t+1)+\eta, \\
x_2(t+1)&=x_2(t)+\dfrac{1}{1+\exp(-u(t))}-0.5+\eta.
\end{align*}
where $\eta\sim\mathcal{N}(0,0.02)$.

The reward function is:

\begin{equation*}
r(t)=\begin{cases}
1 & \left\Vert x\right\Vert <0.1\\
0 & \textit{otherwise}
\end{cases}.
\end{equation*}

The trajectories have an horizon of 80 control steps. The discount factor used in the experiment involving this non linear system is $\gamma=0.95$.

\section{Ship Steering}
\label{a:ship_steering_description}
\begin{table}[b]
  \centering
  \begin{tabular}{c r r}
     \toprule
     Variable & Min & Max \\
     \midrule
     $x$ & 0 & 150 \\ 
     $y$ & 0 & 150 \\
     $\theta$ & $-\pi$ & $\pi$ \\
     $\dot{\theta}$ & $-\frac{\pi}{12}$ & $\frac{\pi}{12}$ \\
     $u$ & $-\frac{\pi}{12}$ & $\frac{\pi}{12}$ \\
     \bottomrule
  \end{tabular}
  \caption{State and action ranges for small and difficult environment}
  \label{tab:ship_steering_small_ranges}
\end{table}

The ship steering problem consist in driving a ship towards a gate. Similar environments have been used both in control theory~\cite{Anderson1990} and reinforcement learning~\cite{ghavamzadeh2003hier}. The state dynamics for the environment is the following:
\begin{eqnarray*}
x(t+1) & = & x(t) + v\sin(\theta(t))dt \\
y(t+1) & = & y(t) + v\cos(\theta(t))dt \\
\theta(t+1) & = & \theta(t) + \omega(t)dt \\
\omega(t+1) & = & \omega(t) + (u - \omega(t))dt/T
\end{eqnarray*}
With $v = 3.0m/s$, $dt = 0.2s$, $T = 5.0$ and $u\in[-\frac{\pi}{12}, \frac{\pi}{12}]$ (radians/s).
The transition model applies the above dynamics for $n$ steps. The reward function is:
\begin{equation*}
r(t)=\begin{cases}
r_{\text{out}} & \textit{outOfBounds}\\
0 & \textit{throughGate}\\
-1 & \textit{otherwise}
\end{cases},
\end{equation*}
where \textit{throughGate} is true if the ship crossed the gate in the last transition, and \textit{outOfBounds} is true if the ship goes out of the environment bounds.
If the ship crosses the gate or goes outside the field, the trajectory ends. The trajectory horizon is set to 5000 control steps. The discount factor used in the experiments on all variants of this environment is $\gamma = 0.99$.

Three different versions of this environment are used in this work: the small environment, the big environment and the difficult environment.
Both the small and the difficult environment share the same state and action space, reported in \cref{tab:ship_steering_small_ranges}. The gate is a line connecting the two points $g_s=(100, 120)$ and $g_e=(120, 100)$.
The difference between these two environments are the reward and the integration steps. In the small environment we have $n=3$ and $r_{\text{out}}=-100$, while in the difficult environment $n=1$ and  $r_{\text{out}}=-10000$.

\begin{table}[t]
	\centering
	\begin{tabular}{c r r}
		\toprule
		Variable & Min & Max \\
		\midrule
		$x$ & 0 & 1000 \\ 
		$y$ & 0 & 1000 \\
		$\theta$ & $-\pi$ & $\pi$ \\
		$\dot{\theta}$ & $-\frac{\pi}{12}$ & $\frac{\pi}{12}$ \\
		$u$ & $-\frac{\pi}{12}$ & $\frac{\pi}{12}$ \\
		\bottomrule
	\end{tabular}
	\caption{State and action ranges for big environment}
	\label{tab:ship_steering_big_ranges}
\end{table}
The state bounds for the ship steering environment are reported in \cref{tab:ship_steering_big_ranges}. The integration steps and the out of bounds penalty for this version of the environment are the same as  the small environments \ie $n=3$ and $r_{\text{out}}=-100$.
\section{Segway}
\label{a:segway_description}
\begin{figure}[b]
	\centering
	\includegraphics{images/env/segway}
	\label{fig:segway_vars}
	\caption{The Segway environment and state variables}
\end{figure}

The Segway is a balancing wheeled robot, that can be described as an inverted pendulum mounted on a wheel, as shown in \cref{fig:segway_vars}. The objective of this environment is to drive the Segway towards the goal position, while maintaining vertical the inverted pendulum as much as possible. This environment is very similar to the Segway described in~\cite{jia2015deep}, however differently from the previous work, we added the position control.
The system dynamics are the following:
\begin{eqnarray*}
\dot{\alpha} & = &\omega_{\alpha} \\
\dot{\beta} & = &\omega_{\beta} \\
\dot{s} & = & -\omega_{\beta}r \\
\dot{\omega_{\alpha}} & = &-\dfrac{h_2 l M_p r\omega_{\alpha}^2\sin\alpha - g h_1 l M_p\sin\alpha + (h_2+h_1)u}{h_1h_3-h_2^2} \\
\dot{\omega_{\beta}} & = &\dfrac{h_3 l M_p r\omega_{\alpha}^2\sin\alpha - g h_2 l M_p\sin\alpha + (h_3+h_2)u}{h_2h_3-h_2^2}\\
h_1 & = & (M_r+M_p)r^2+I_r \\
h_2 & = & M_prl\cos(\alpha) \\
h_3 & = & l^2M_p+I_p.
\end{eqnarray*}
The control action $u$ is constrained such that the applied torque falls in the interval $[-5, 5]$.
The discount factor used in the experiments involving this environment is $\gamma = 0.99$.
The values for the parameters of the system can be found in \cref{tab:segway_parameters}.
\begin{table}[t]
	\centering
	\begin{tabular}{c l}
		\toprule
		Parameter & value\\
		\midrule
		$M_r$ & $0.3*2$ \\
		$Mp$ & $2.55$ \\
		$I_p$ & $2.6\cdot 10^{-2}$ \\
		$I_r$ & $4.54\cdot10^{-4} * 2$ \\
		$l$ & $13.8\cdot 10{-2}$ \\
		$r$ & $5.5\cdot 10^{-2}$ \\
		$dt$ & $10^{-2}$ \\
		$g$ & 9.81 \\
		\bottomrule
	\end{tabular}
	\caption{Parameters used in the Segway environment}
	\label{tab:segway_parameters}
\end{table}

The transition model is obtained by integrating the system dynamics, applying constant torque action $u$, for $\Delta t=0.02$.
The initial state is $x_0=[-1.0, \frac{\pi}{8}, 0, 0]$. 
The discount factor for every experiment using this environment is set to $\gamma=0.9$.
All the states where $\alpha\not\in[-\frac{\pi}{2}, \frac{\pi}{2}]$ or $s\not\in[-2, 2]$ are absorbing states. All trajectories that doesn't reach an absorbing state are cut using an horizon of 1500 control steps.
The reward function is the following: 
\begin{equation*}
	r(t)=\begin{cases}
		-10000 & \textit{fall}\\
		-10000 & \textit{out}\\
		-x(t)Qx(t)^T, & \textit{otherwise}
	\end{cases},
\end{equation*}
where $Q=diag([10, 3, 0.1, 0.1])$ is a diagonal weights matrix for the quadratic cost penalty,  \textit{fall} and \textit{out} are true when the pendulum angle $\alpha$  and the position variable $s$ go outside their bounds.

\section{Prey-Predator}
\label{a:prey_predator_description}
\begin{table}[b]
	\centering
	\begin{tabular}{c r r}
		\toprule
		Variable & Min & Max \\
		\midrule
		$x_{\text{prey, predator}}$ & -5 & 5 \\ 
		$y_{\text{prey, predator}}$ & -5 & 5 \\
		$\theta_{\text{prey, predator}}$ & $-\pi$ & $\pi$ \\
		$v_{\text{prey}}$ & 0 & 1.3 \\
		$v_{\text{predator}}$ & 0 & 1.0 \\
		$\omega_{\text{prey}}$ & -2.1666 & 2.1666 \\
		$\omega_{\text{predator}}$ & -1.666 & 1.666 \\
		\bottomrule
	\end{tabular}
	\caption{State and action ranges for prey-predator environment}
	\label{tab:segway_ranges}
\end{table}
The prey-predator environment consist in two differential drive robots, where the objective of the learning agent, the predator, is to catch the other robot, the prey, inside a square environment with obstacles. Both robots are considered as points, with no footprint.
The prey has a higher maximum angular velocity than the predator but they have the same turning radius ($0.6m$).
The differential drive dynamics used are the following:
\begin{align*}
 x(t+1) & = x(t)+\cos(\theta(t))v(t)dt \\
 y(t+1) & = y(t)+\sin(\theta(t))v(t)dt \\
 \theta(t+1) & = \theta(t)+\omega(t) dt.
\end{align*}
In our experiments we set $dt=0.1$.
State and action are bounded as described in~\cref{tab:segway_ranges}.
The environment is constituted by the following obstacles: the segment $(1, 3.48)-(5, 3.48)$ and the polygonal chain $(-3.0, -1.5)-(-3.0, 1.25)-(-1.48 ,1.25)-(-1.48, -1.5)$. In~\cref{fig:prey_predator_example} a graphical representation of this environment is shown.

The prey is considered captured if the predator distance is less than $0.4m$ and there is no obstacle between the two robots. If the prey has been captured, then the episode ends. All episodes are cut after an horizon of 500 steps.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.5\textwidth]{images/env/prey_predator}
    \caption[The Prey-Predator environment]{The prey-predator environment. Predator is shown in red, prey in blue. The white area around the predator represents the catch radius.}
    \label{fig:prey_predator_example}
\end{figure}


The evasion policy of the prey is a complex policy. The linear velocity is calculated as follows:
\begin{equation*}
v_{\text{prey}}(t)=\begin{cases}
0 & \text{distance} > 3.0\\
1.3 & \text{distance} \leq 1.5\\
0.65 & \textit{otherwise}
\end{cases}
\end{equation*}
To compute the angular velocity, first is computed the difference between the current prey angle and the angle \wrt the predator. then the value of the angular velocity is computed by multiplying the error by a proportional gain $k=\frac{1}{\pi}$. If a potential collision is detected within $1.5m$, the prey rotates in the direction of the biggest angle at maximum angular velocity. If following this rotation, the prey is closer than $0.9m$ to another obstacle, the rotation is inverted. This last behaviour is implemented in order to maneuver properly near the corners, avoiding to get trapped easily.

The reward function for this environment is computed by multiplying by -1 the distance of the prey and the predator after applying the action \ie is computed using the next state. For every experiment using this environment the discount factor is set to $\gamma=0.99$.


